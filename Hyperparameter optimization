

grid search
random search


Handing tuning will help you learn the basics, for example, in gradient boosting, when you increase the depth, you should reduce the learning rate. It won't be possible to learn this if you use automated tools.

Once you get better with hand-tuning the parameters, you might not even need any automated hyper-parameter tuning. When you create large models or introduce a lot of features, you also make it susceptible to overfitting
the training data. To avoid overfitting, you need to introduce noise in training data features or penalize the cost function. This penalization is called regularization and helps with generalizing the model.


Model                 Optimize                    Range of values
Random Forest         n_estimators                120, 300, 500, 800, 1200
                      max_depth                   5, 8, 15, 25, 30, None
                      min_samples_split           1,2,5,10,15,100
                      min_samples_leaf            1,2,5,10
                      max features                log2, sqrt, None

XGBoost               eta                         0.01, 0.015, 0.025, 0.05, 0.1
                      gamma                       0.05-0.1, 0.3, 0.5, 0.7, 0.9, 1.0
                      max_depth                   3,5,7,9,12,15,17,25
                      min_child_weight            1,3,5,7
                      subsample                   0.6,0.7,0.8,0.9,1.0
                      colsample_bytree            0.6,0.7,0.8,0.9,1.0
                      lambda                      0.01-0.1, 1.0, RS*
                      alpha                       0, 0.1, 0.5, 1.0 RS*

RS* - Random Search




