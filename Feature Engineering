Feature engineering is one of the most crucial parts of building a good machine learning model. If we have useful features, the model will perform better.
If we have useful features, the model will perform better. There are many situations where you can avoid large, complicated models and use simple models with crucially engineered features.

We must keep in mind that feature engineering is something that is done in the best possible manner only when you have some knowledge about the domain of the problem and depends a lot on the data in concern.
However, there are some general techniques that you can try to create features from almost all kinds of numerical and categorical variables. Feature engineering is not just about creating new features from data but also 
includes different types of normalization and transformations.

Dealing with date and time data 
- dataframe with a datatime type column, using this column, we can create features like:
-- Year 
-- Week of Year
-- Month
-- Day of week
-- Weekend
-- Hour
-- Any many more

Date time features are critical when you are dealing with time-series data, for example, predicting sales of a store but would like to use a model like XGBoost on aggregated features.

binning, log transformation to reduce variance, take exponential

Missing value
for categorical features, let's keep it super simple. If you ever encounter missing values in categorical features, treat it as a new category! As simple as this is, it (almost) always works!

One way to fill missing values in numerical data would be to choose a value that does not appear in the specific feature and fill using that. For example, let's say 0 is not seen in the feature. So,
we fill all the missing values using 0. This is one of the ways but might not be the most effective. One of the methods that works better than filling 0s for numerical data is to fill with mean instead.
You can also try to fill with the median of all the values for that feature, or you can use the most common value to fill the missing values. 

A fancy way of filling in the missing values would be to use a k-nearest neighbour method. You can select a sample with missing values and find the nearest neighbours utilising some kind of distance metric,
for example, Euclidean distance. Then you can take the mean of all nearest neighbours and fill up the missing value. You can use the KNN imputer implementation for filling missing values like this.

Another way of imputing missing values in a column would be to train a regression model that tries to predict missing values in a column based on other columns. So you start with one column that has a missing value
and treat this column as the target column for regression model without the missing values. Using all the other columns, you now train a model on samples for which there is no missing value in the concerned column
and then try to predict target (the same column) for the samples that were removed earlier. This way, you have a more robust model based imputation.

Always remember that imputing values for tree-based models is unneccessary as they can handle it themselves.

And always remember to scale or normalize your features if you are using linear models like logistic regression or a model like SVM. Tree-based models will always work fine without any normalization of features.




